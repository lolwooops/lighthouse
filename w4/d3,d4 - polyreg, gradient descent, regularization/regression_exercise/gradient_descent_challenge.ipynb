{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Train the 2nd order polynomial predictor using both gradient descent and stochastic gradient descent. Optimize the stepsizes and compare against scikit-learn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download data from https://drive.google.com/file/d/0Bz9_0VdXvv9bUUNlUTVrMF9VcVU/view?usp=sharing.\n",
    "2. Create a function psi(x), which transforms features AST (assists), REB (rebounds) and STL (steals) into 2nd order polynomial features (add each feature squared and each pair of features multiplied with every other)\n",
    "3. Create a transformed data matrix X, where each x is mapped to psi(x).\n",
    "4. Create a function p2(x,w), which outputs the value of the polynomial at x for given parameters w.\n",
    "5. Create a function Loss(X,y,w), which computes the squared loss of predicting y from X by p2(x,w) using parameters w. Take variable PTS as y. We will predict scored points based on assists, rebounds and steals.\n",
    "6. Code up the gradient descent. It should input a point w and a stepsize.\n",
    "7. Choose an arbitrary point and stepsize. Run gradient descent for 100 iterations and compute the Loss after each iteration. How does the loss behave? Does it converge to something?\n",
    "8. Can you find the stepsize, for which the loss is smallest after 100 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = pd.read_csv('C:/Users/Tim/Desktop/lighthouse/w4/d3,d4/regression_exercise/nba1315.csv', delimiter=';')\n",
    "x = nb[['AST','REB','STL']]\n",
    "y = nb['PTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn polynomialfeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "x_poly = poly.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 4.100e+01, 4.300e+01, ..., 1.849e+03, 6.020e+02,\n",
       "        1.960e+02],\n",
       "       [1.000e+00, 2.300e+01, 4.300e+01, ..., 1.849e+03, 3.440e+02,\n",
       "        6.400e+01],\n",
       "       [1.000e+00, 2.000e+01, 3.900e+01, ..., 1.521e+03, 2.730e+02,\n",
       "        4.900e+01],\n",
       "       ...,\n",
       "       [1.000e+00, 2.300e+01, 5.200e+01, ..., 2.704e+03, 4.160e+02,\n",
       "        6.400e+01],\n",
       "       [1.000e+00, 2.300e+01, 4.100e+01, ..., 1.681e+03, 4.510e+02,\n",
       "        1.210e+02],\n",
       "       [1.000e+00, 1.700e+01, 4.400e+01, ..., 1.936e+03, 1.760e+02,\n",
       "        1.600e+01]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7380, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7380\n",
      "(7380,)\n"
     ]
    }
   ],
   "source": [
    "print(x_poly.shape[0])\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2(x,w):\n",
    "    y_hat = x.dot(w)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(x,y,w):\n",
    "    n = x.shape[0]\n",
    "    loss_ = (-2/n)*sum(x.T.dot(y-p2(x,w)))\n",
    "    return loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(stepsize,epoch):\n",
    "    w = np.random.uniform(size=(x_poly.shape[1],))\n",
    "    loss_history = {}\n",
    "    w_history = []\n",
    "    x = x_poly\n",
    "    for i in range(epoch+1):\n",
    "        w_grad = Loss(x,y,w)\n",
    "        w = w - stepsize*w_grad\n",
    "        loss_history[i] = Loss(x,y,w)\n",
    "        w_history.append(w)\n",
    "    print(\"Learning Rate: {}\".format(stepsize))\n",
    "    print(\"Number of Iterations: {}\".format(epoch))\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 0.01\n",
      "Number of Iterations: 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: -9314576524015.377,\n",
       " 1: 3.203555886792451e+18,\n",
       " 2: -1.1017967691115647e+24,\n",
       " 3: 3.789402037372142e+29,\n",
       " 4: -1.3032864320721323e+35,\n",
       " 5: 4.482384047065151e+40,\n",
       " 6: -1.5416232572481934e+46,\n",
       " 7: 5.302094247913927e+51,\n",
       " 8: -1.823545621901321e+57,\n",
       " 9: 6.271707894411343e+62,\n",
       " 10: -2.1570241753430673e+68,\n",
       " 11: 7.418638385822247e+73,\n",
       " 12: -2.551487189096615e+79,\n",
       " 13: 8.775312311441903e+84,\n",
       " 14: -3.0180871176785637e+90,\n",
       " 15: 1.038008623125641e+96,\n",
       " 16: -3.570015906339863e+101,\n",
       " 17: 1.2278331111683805e+107,\n",
       " 18: -4.2228779603031395e+112,\n",
       " 19: 1.4523715076102468e+118,\n",
       " 20: -4.99513132026823e+123,\n",
       " 21: 1.7179720736727963e+129,\n",
       " 22: -5.908609517319198e+134,\n",
       " 23: 2.0321440006599477e+140,\n",
       " 24: -6.989138861374385e+145,\n",
       " 25: 2.4037697135493333e+151,\n",
       " 26: -8.267268615465469e+156,\n",
       " 27: 2.843355999329077e+162,\n",
       " 28: -9.779134699696047e+167,\n",
       " 29: 3.3633310601051975e+173,\n",
       " 30: -1.156748134394749e+179,\n",
       " 31: 3.978395889412921e+184,\n",
       " 32: -1.368286957400557e+190,\n",
       " 33: 4.705939906017622e+195,\n",
       " 34: -1.6185106698028764e+201,\n",
       " 35: 5.566532596211073e+206,\n",
       " 36: -1.9144937208510506e+212,\n",
       " 37: 6.584505064558365e+217,\n",
       " 38: -2.2646042905756736e+223,\n",
       " 39: 7.788637934987631e+228,\n",
       " 40: -2.6787408791364443e+234,\n",
       " 41: 9.212975050904187e+239,\n",
       " 42: -3.168612162141852e+245,\n",
       " 43: 1.0897785979663429e+251,\n",
       " 44: -3.748068024149433e+256,\n",
       " 45: 1.2890704533807799e+262,\n",
       " 46: -4.4334911294904465e+267,\n",
       " 47: 1.5248075497906322e+273,\n",
       " 48: -5.24426010110397e+278,\n",
       " 49: 1.8036547636327804e+284,\n",
       " 50: -6.203297402602888e+289,\n",
       " 51: 2.133495802025578e+295,\n",
       " 52: -7.337717413565954e+300,\n",
       " 53: inf,\n",
       " 54: nan,\n",
       " 55: nan,\n",
       " 56: nan,\n",
       " 57: nan,\n",
       " 58: nan,\n",
       " 59: nan,\n",
       " 60: nan,\n",
       " 61: nan,\n",
       " 62: nan,\n",
       " 63: nan,\n",
       " 64: nan,\n",
       " 65: nan,\n",
       " 66: nan,\n",
       " 67: nan,\n",
       " 68: nan,\n",
       " 69: nan,\n",
       " 70: nan,\n",
       " 71: nan,\n",
       " 72: nan,\n",
       " 73: nan,\n",
       " 74: nan,\n",
       " 75: nan,\n",
       " 76: nan,\n",
       " 77: nan,\n",
       " 78: nan,\n",
       " 79: nan,\n",
       " 80: nan,\n",
       " 81: nan,\n",
       " 82: nan,\n",
       " 83: nan,\n",
       " 84: nan,\n",
       " 85: nan,\n",
       " 86: nan,\n",
       " 87: nan,\n",
       " 88: nan,\n",
       " 89: nan,\n",
       " 90: nan,\n",
       " 91: nan,\n",
       " 92: nan,\n",
       " 93: nan,\n",
       " 94: nan,\n",
       " 95: nan,\n",
       " 96: nan,\n",
       " 97: nan,\n",
       " 98: nan,\n",
       " 99: nan,\n",
       " 100: nan}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent(0.01,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
