LDA 
goal is to project dataset onto lower dimensional space with good class separability in order to avoid overfitting + reduce comp cost
basically project a feature space (dataset n dimensional samples) onto a small subspace k while maintaining class disciminatory information

both pca and lda are linear transformation techniques that are commonly used for dimensionality reduction
-pca more unsupervised since it ignores class labels to max variance in dataset
-lda more supervised and computes directions (linear discriminants) that will represent axes that maximize separationg btw multiple classes

pce tends to outperform lda if number of samples per class is relatively small; not uncommon to use both lda and pca combo
(pca DR then lda)

good feature subspace?
assume goal = reduce dimensions of d dimensional dataset by projecting onto kD subspace (k < d)
compute eigenvectors (components) and collect them in "scatter matrices"
-ea EVec is associated with ev --> length/magnitude of evec
-all ev have a similar magnitude then this is a good indicator that data is already on a good feature space
-if some ev bigger than others, keep only evec with highest ev since they contain more information about our data distribution

LDA
-compute d dimensional mean vectors for different classes
-compute scatter matrices
-compute eigenvectors and eigenvalues
-sort evec by decreasing ev and choose k evec with largest ev to for a dxk dimensional matrix w
-use W matrix to transform samples onto new subspace -> Y = X W

pca finds axes with maximum variance for the whole dataset whereas LDa tries to find axes for best class separability

feature scaling such as standardization does not change overall results of lda; scatter matrices will be different and evec will be as well
however, most important part is ev will be exactly the same
