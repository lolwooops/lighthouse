{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the walkthrough, they used the .predict() method to get the predicted label of each observation. In every classificator from sklearn, this method returns the actual class label \n",
    "for each observation based on the probability cut-off 0.5. When the probability is higher than 0.5 .predict() always returns 1. Sometimes, it's better to use the .predict_proba() \n",
    "method and apply a custom cut-off to decide between the labels. For example, in credit scoring, we can disapprove loans when the probability of default is higher than 0.2.\n",
    "The cut-off doesn't always need to be 0.5.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Properties of Logistic Regression:\n",
    "\n",
    "The dependent variable in logistic regression follows Bernoulli Distribution.\n",
    "Estimation is done through maximum likelihood.\n",
    "No R Square, Model fitness is calculated through Concordance, KS-Statistics.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "The MLE is a \"likelihood\" maximization method, while OLS is a distance-minimizing approximation method. Maximizing the likelihood function determines the parameters that are most \n",
    "likely to produce the observed data. From a statistical point of view, MLE sets the mean and variance as parameters in determining the specific parametric values for a given model. \n",
    "This set of parameters can be used for predicting the data needed in a normal distribution.\n",
    "\n",
    "Ordinary Least squares estimates are computed by fitting a regression line on given data points that has the minimum sum of the squared deviations (least square error). \n",
    "Both are used to estimate the parameters of a linear regression model. MLE assumes a joint probability mass function, while OLS doesn't require any stochastic assumptions for\n",
    "minimizing distance.\n",
    "\"\"\"\n",
    "\n",
    "#import pandas\n",
    "import pandas as pd\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Feature\n",
    "# Here, you need to divide the given columns into two types of variables dependent(or target variable) and independent variable(or feature variables).\n",
    "\n",
    "#split dataset in features and target variable\n",
    "feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']\n",
    "X = pima[feature_cols] # Features\n",
    "y = pima.label # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Development and Prediction\n",
    "# First, import the Logistic Regression module and create a Logistic Regression classifier object using LogisticRegression() function.\n",
    "\n",
    "# Then, fit your model on the train set using fit() and perform prediction on the test set using predict().\n",
    "\n",
    "# import the class\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate the model (using the default parameters)\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model with data\n",
    "logreg.fit(X_train,y_train)\n",
    "\n",
    "#\n",
    "y_pred=logreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation using Confusion Matrix\n",
    "# A confusion matrix is a table that is used to evaluate the performance of a classification model. You can also visualize the performance of an algorithm. The fundamental of a \n",
    "# confusion matrix is the number of correct and incorrect predictions are summed up class-wise.\n",
    "\n",
    "# import the metrics class\n",
    "from sklearn import metrics\n",
    "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Here, you can see the confusion matrix in the form of the array object. The dimension of this matrix is 2*2 because this model is binary classification. \n",
    "You have two classes 0 and 1. Diagonal values represent accurate predictions, while non-diagonal elements are inaccurate predictions. In the output, 119 and 36 are \n",
    "actual predictions, and 26 and 11 are incorrect predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Confusion Matrix using Heatmap\n",
    "# Let's visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn.\n",
    "\n",
    "# Here, you will visualize the confusion matrix using Heatmap.\n",
    "\n",
    "# import required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "class_names=[0,1] # name  of classes\n",
    "fig, ax = plt.subplots()\n",
    "tick_marks = np.arange(len(class_names))\n",
    "plt.xticks(tick_marks, class_names)\n",
    "plt.yticks(tick_marks, class_names)\n",
    "# create heatmap\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "ax.xaxis.set_label_position(\"top\")\n",
    "plt.tight_layout()\n",
    "plt.title('Confusion matrix', y=1.1)\n",
    "plt.ylabel('Actual label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Evaluation Metrics\n",
    "# Let's evaluate the model using model evaluation metrics such as accuracy, precision, and recall.\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Well, you got a classification rate of 80%, considered as good accuracy.\n",
    "\n",
    "Precision: Precision is about being precise, i.e., how accurate your model is. In other words, you can say, when a model makes a prediction, how often it is correct. \n",
    "In your prediction case, when your Logistic Regression model predicted patients are going to suffer from diabetes, that patients have 76% of the time.\n",
    "\n",
    "Recall: If there are patients who have diabetes in the test set and your Logistic Regression model can identify it 58% of the time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "# Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. It shows the tradeoff between sensitivity and specificity.\n",
    "\n",
    "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "AUC score for the case is 0.86. AUC score 1 represents perfect classifier, and 0.5 represents a worthless classifier.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advantages\n",
    "\n",
    "Because of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. \n",
    "Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Disadvantages\n",
    "\n",
    "Logistic regression is not able to handle a large number of categorical features/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the \n",
    "logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated \n",
    "to the target variable and are very similar or correlated to each other.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
