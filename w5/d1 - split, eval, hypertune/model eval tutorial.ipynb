{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will generate 10 observations (y_true) and 10 predictions (y_pred) from a model.\n",
    "\n",
    "# generate 'ground truth'\n",
    "y_true = np.random.normal(0,1,10)\n",
    "\n",
    "# generate random errors\n",
    "errors = np.random.normal(0,0.02,10)\n",
    "\n",
    "# simulate predictions\n",
    "y_pred = y_true + errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mean Squared Error (MSE) / Root Means Squared Error (RMSE)\n",
    "\n",
    "# import MSE from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# compute MSE\n",
    "MSE = mean_squared_error(y_true,y_pred)  \n",
    "\n",
    "# print MSE\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "All regression evaluation functions from sklearn.metrics take two mandatory arrays as parameters. \n",
    "The first is an array with ground truth values (in our case y_true variable) and the second is our prediction (in our case y_pred variable).\n",
    "\n",
    "To get RMSE from MSE we have to options: the first option is to compute the square root from MSE by Numpy and the second option is to use the squared=False option in a function. \n",
    "Let's try both options\n",
    "\"\"\"\n",
    "\n",
    "# RMSE by Numpy\n",
    "RMSE = np.sqrt(MSE)\n",
    "print(RMSE)\n",
    "\n",
    "# RMSE by sklearn\n",
    "RMSE = mean_squared_error(y_true,y_pred,squared=False)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will use the same principle as in regression model evaluation and use synthetic data. With the only difference that we will need predicted probabilities instead of predicted \n",
    "labels (predicted values in regression). The important thing to mention is that we are simulating the behavior of a binary classifier. \n",
    "It means that the predicted class is only positive (returns 1) or negative (returns 0).\n",
    "\"\"\"\n",
    "\n",
    "# ground truth\n",
    "y_true = [1,1,0,1,0,0,1,0,0,1]\n",
    "\n",
    "# simulate probabilites of positive class\n",
    "y_proba = [0.9,0.7,0.2,0.99,0.7,0.1,0.5,0.2,0.4,0.6]\n",
    "\n",
    "# set the threshold to predict positive class\n",
    "thres = 0.5\n",
    "\n",
    "# class predictions\n",
    "y_pred = [int(value > thres) for value in y_proba]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the variable y_proba, we have the probabilities of the observations from the positive class. We also set the threshold value. All observations with probabilities above this \n",
    "threshold are assigned to the positive class and stored in the y_pred variable.\n",
    "\"\"\"\n",
    "\n",
    "#accuracy\n",
    "# import accuracy_score from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# compute accuracy\n",
    "accuracy = accuracy_score(y_true,y_pred)\n",
    "\n",
    "# print accuracy\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "#f1\n",
    "# import f1_score from sklearn\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# compute F1-score\n",
    "f1_score = f1_score(y_true,y_pred)\n",
    "\n",
    "# print F1-score\n",
    "print(f1_score)\n",
    "\n",
    "\n",
    "#auc score\n",
    "# import roc_auc_score from sklearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# compute AUC-score\n",
    "auc = roc_auc_score(y_true,y_proba)\n",
    "\n",
    "# print AUC-score\n",
    "print(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In roc_auc_score we use probabilities (y_proba) instead of class labels. If we passed class labels no errors would be shown but the score would be inaccurate. Be careful and \n",
    "read the documentation before using Sklearn functions.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
