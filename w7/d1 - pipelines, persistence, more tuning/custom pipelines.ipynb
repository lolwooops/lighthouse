{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will cover two ways of customization:\n",
    "\n",
    "Putting our own classes into a pipeline.\n",
    "Using ColumnTransformer from sklearn.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "In addition to scoring, you can (and should) include all preprocessing steps into a single Pipeline:\n",
    "\n",
    "Selecting columns from Pandas dataframes\n",
    "\n",
    "Cleaning/preprocessing/normalizing data\n",
    "\n",
    "Imputting missing data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Transformer that applies an arbitrary function to a pandas dataframe:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class DataframeFunctionTransformer():\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def transform(self, input_df, **transform_params):\n",
    "        return self.func(input_df)\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# this function takes a dataframe as input and\n",
    "# returns a modified version thereof\n",
    "def process_dataframe(input_df):\n",
    "    input_df[\"text\"] = input_df[\"text\"].map(lambda t: t.upper())\n",
    "    return input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"id\":[1,2,3,4],\n",
    "    \"text\":[\"foo\",\"Bar\",\"BAz\",\"quux\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this pipeline has a single step\n",
    "pipeline = Pipeline([\n",
    "    (\"uppercase\", DataframeFunctionTransformer(process_dataframe))\n",
    "])\n",
    "\n",
    "# apply the pipeline to the input dataframe\n",
    "pipeline.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For example, you may need to add a step that turns a sparse matrix into a dense matrix, if you need to use a method that requires dense matrices such as \n",
    "GaussianNB or PCA:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.sparse.csr_matrix import todense\n",
    "\n",
    "class ToDenseTransformer():\n",
    "\n",
    "    # here you define the operation it should perform\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()\n",
    "\n",
    "    # just return self\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to make matrices dense because PCA does not work with sparse vectors.\n",
    "pipeline = Pipeline([\n",
    "    ('to_dense',ToDenseTransformer()),\n",
    "    ('pca',PCA()),\n",
    "    ('clf',DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "pipeline.fit(sparse_data_matrix,target)\n",
    "pipeline.predict(sparse_data_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can create a custom transformer that can go into scikit learn pipelines.\n",
    "\n",
    "It just needs to implement fit and transform:\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class SelectColumnsTransformer():\n",
    "    def __init__(self, columns=None):\n",
    "        self.columns = columns\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        cpy_df = X[self.columns].copy()\n",
    "        return cpy_df\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'name':['alice','bob','charlie','david','edward'],\n",
    "    'age':[24,32,np.nan,38,20]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single transformer\n",
    "pipe = Pipeline([\n",
    "    ('selector', SelectColumnsTransformer([\"name\"]))\n",
    "])\n",
    "\n",
    "pipe.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use ColumnTransformer passing a SimpleImputer\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'name':['alice','bob','charlie','david','edward'],\n",
    "    'age':[24,32,np.nan,38,20]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_step = ColumnTransformer([\n",
    "        ('impute_mean', SimpleImputer(strategy='mean'), ['age'])\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('select', select)\n",
    "])\n",
    "\n",
    "# fit as you would a normal transformer\n",
    "pipe.fit(features)\n",
    "\n",
    "# transform the input\n",
    "pipe.transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A FunctionTransformer can be to apply an arbitrary function to the input data. You can also pass parameters using kw_args with a python dict.\n",
    "\n",
    "As an example, create a custom FunctionTransformer that applies stemming to some text, taking an nltk stemmer as argument:\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import RSLPStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# dummy dataframe\n",
    "df = pd.DataFrame({\n",
    "    'text':[\n",
    "        'Lorem ipsum dolor sit amet, consectetur adipiscing elit.',\n",
    "        'Sed accumsan congue enim non pretium.',\n",
    "        'In hac habitasse platea dictumst.',\n",
    "        'Sed tincidunt ipsum nec urna vulputate luctus.'\n",
    "    ],\n",
    "    'target':[0, 1, 0, 1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes a dataframe row and a stemmer\n",
    "def stem_str(input_series, stemmer):\n",
    "\n",
    "    def stem(input_str):\n",
    "        return \" \".join([stemmer.stem(t) for t in input_str.split(\" \")]).strip()\n",
    "\n",
    "    return input_series.apply(stem)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('stemmer', FunctionTransformer(\n",
    "        func=stem_str,                        # function to be used\n",
    "        kw_args={'stemmer': RSLPStemmer()})), # parameters to the function\n",
    "    ('vect', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# now use it like you would any pipeline\n",
    "pipeline.fit(df[\"text\"],df[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a single Pipeline that takes a DataFrame as input, does preprocessing (for all columns) using a ColumnTransformer and trains a \n",
    "DecisionTreeClassifier on top of it.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# this is the input dataframe\n",
    "df = pd.DataFrame({\n",
    "    'favorite_color':['blue','green','red','green','blue'],\n",
    "    'age': [10,15,10,np.nan,10],\n",
    "    'target':[1,0,1,0,1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define individual transformers in a pipeline\n",
    "categorical_preprocessing = Pipeline([('ohe', OneHotEncoder())])\n",
    "numerical_preprocessing = Pipeline([('imputation', SimpleImputer())])\n",
    "\n",
    "# define which transformer applies to which columns\n",
    "preprocess = ColumnTransformer([\n",
    "    ('categorical_preprocessing', categorical_preprocessing, ['favorite_color']),\n",
    "    ('numerical_preprocessing', numerical_preprocessing, ['age'])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the final pipeline with preprocessing steps and \n",
    "# the final classifier step\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('clf', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# now fit the pipeline using the whole dataframe\n",
    "df_features = df[['favorite_color','age']]\n",
    "df_target = df['target']\n",
    "\n",
    "# call fit on the dataframes\n",
    "pipeline.fit(df_features, df_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In the previous sections, we have seen what we can do with Pipelines:\n",
    "\n",
    "Tune parameters of ALL steps.\n",
    "Combine features from different pipelines using FeatureUnion.\n",
    "Select a subset of features for pipeline steps with ColumnTransformer.\n",
    "Create our own transformators and include them in a pipeline.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
