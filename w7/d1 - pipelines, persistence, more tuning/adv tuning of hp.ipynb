{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "data = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, QuantileTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The pipeline we are going to setup is composed of the following tasks:\n",
    "\n",
    "Data Normalization: in this tutorial we have selected three different normalization methods, including the QuantileTransformer (check out the documentation)..\n",
    "Dimensionality Reduction: we selected Principal Component Analysis (PCA) and a univariate feature selection algorithm as possible candidates.\n",
    "Regression: we apply a simple regularized linear method, although the method is easily extendable to other learning algorithms.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "ridge.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reduce_dim', PCA()),\n",
    "        ('regressor', Ridge())\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipe.fit(X_train, y_train)\n",
    "print('Testing score: ', pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pipe.steps[1][1].explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hyper-parameters are parameters that are manually tuned by a human operator to maximize the model performance against a validation set through a grid search.\n",
    "\"\"\"\n",
    "\n",
    "#Concerning PCA, we want to evaluate how accuracy varies with the number of components, from 1 to 10:\n",
    "import numpy as np\n",
    "n_features_to_test = np.arange(1, 11)\n",
    "\n",
    "alpha_to_test = 2.0**np.arange(-6, +6)\n",
    "\n",
    "params = {'reduce_dim__n_components': n_features_to_test,\\\n",
    "              'regressor__alpha': alpha_to_test}\n",
    "\n",
    "\"\"\"\n",
    "It is worth remarking the convention adopted to name the parameters: name of the pipeline step, followed by a double underscore (__), then finally the name \n",
    "of the parameter within the step. \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can follow the same approach, this time to decide which algorithm we should use, for example, to perform data normalization:\n",
    "\"\"\"\n",
    "\n",
    "scalers_to_test = [StandardScaler(), RobustScaler(), QuantileTransformer()]\n",
    "\n",
    "params = {'scaler': scalers_to_test,\n",
    "        'reduce_dim__n_components': n_features_to_test,\\\n",
    "        'regressor__alpha': alpha_to_test}\n",
    "\n",
    "\"\"\"\n",
    "If we wanted to pick between reduce dim methods\n",
    "Luckily, GridSearchCV also allows to optimize lists of parameter dictionaries, which solves this issue as well:\n",
    "\"\"\"\n",
    "\n",
    "params = [\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [PCA()],\n",
    "         'reduce_dim__n_components': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test},\n",
    "\n",
    "        {'scaler': scalers_to_test,\n",
    "         'reduce_dim': [SelectKBest(f_regression)],\n",
    "         'reduce_dim__k': n_features_to_test,\\\n",
    "         'regressor__alpha': alpha_to_test}\n",
    "        ]\n",
    "\n",
    "gridsearch = GridSearchCV(pipe, params, verbose=1).fit(X_train, y_train)\n",
    "print('Final score is: ', gridsearch.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
