{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "df = pd.DataFrame(data['data'])\n",
    "df.columns = data['feature_names']\n",
    "y = data['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Firstly, we will create our own class to keep only features we want in our pipeline. We don't want to run PCA on all features but only on the sample so we create own class \n",
    "that filters the features in the original dataframe. We can put our own classes into the pipelines, as long as they have following methods:\n",
    "\n",
    ".fit()\n",
    ".transform()\n",
    ".fit_transform()\n",
    "\"\"\"\n",
    "# own class that can be inserted to pipeline as any other sklearn object.\n",
    "class RawFeats:\n",
    "    def __init__(self, feats):\n",
    "        self.feats = feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.feats]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "# features we want to keep for PCA\n",
    "feats = ['alcohol','malic_acid','ash','alcalinity_of_ash','magnesium',\n",
    "         'total_phenols','flavanoids','nonflavanoid_phenols']\n",
    "# creating class object with indexes we want to keep.\n",
    "raw_feats = RawFeats(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = SelectKBest(k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "As in the tutorial yesterday we will apply two different feature extraction techniques:\n",
    "\n",
    "PCA\n",
    "SelectKBest\n",
    "and combine them with FeatureUnion. The small difference is that we will use only sample of features for PCA.\n",
    "\"\"\"\n",
    "\n",
    "PCA_pipeline = Pipeline([\n",
    "    (\"rawFeats\", raw_feats),\n",
    "    (\"scaler\", sc),\n",
    "    (\"pca\", pca)\n",
    "])\n",
    "\n",
    "kbest_pipeline = Pipeline([(\"kBest\", selection)])\n",
    "\n",
    "all_features = FeatureUnion([\n",
    "    (\"pcaPipeline\", PCA_pipeline), \n",
    "    (\"kBestPipeline\", kbest_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pipeline = Pipeline([\n",
    "    (\"features\", all_features),\n",
    "    (\"rf\", rf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up our parameters grid\n",
    "param_grid = {\"features__pcaPipeline__pca__n_components\": [1, 2, 3],\n",
    "                  \"features__kBestPipeline__kBest__k\": [1, 2, 3],\n",
    "                  \"rf__n_estimators\":[2, 5, 10],\n",
    "                  \"rf__max_depth\":[2, 4, 6]\n",
    "             }\n",
    "\n",
    "# create a Grid Search object\n",
    "grid_search = GridSearchCV(main_pipeline, param_grid, n_jobs = -1, verbose=10, refit=True)    \n",
    "\n",
    "# fit the model and tune parameters\n",
    "grid_search.fit(df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( grid_search, open( \"model.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new .py file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Flask and jsonify\n",
    "from flask import Flask, jsonify, request\n",
    "# import Resource, Api and reqparser\n",
    "from flask_restful import Resource, Api, reqparse\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import pickle\n",
    "\n",
    "# Create an API similar to previous tutorial.\n",
    "app = Flask(__name__)\n",
    "api = Api(app)\n",
    "\n",
    "\n",
    "# At the beginning of the file, we need to create the same custom class we used in the model creation part. The functions from that \n",
    "# class are used in the model and stored in the pickle file we created earlier. Therefore, the model needs to have access to the class \n",
    "# during the scoring as well. The accesses to other sklearn modules are provided automatically and we don't have to do anything about \n",
    "# them in the scoring file.\n",
    "\n",
    "class RawFeats:\n",
    "    def __init__(self, feats):\n",
    "        self.feats = feats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.feats]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    \n",
    "# load model\n",
    "model = pickle.load( open( \"model.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "# Now, we need to create an endpoint where we can communicate with our ML model. This time, we are going to use POST request.\n",
    "class Scoring(Resource):\n",
    "    def post(self):\n",
    "        json_data = request.get_json()\n",
    "        df = pd.DataFrame(json_data.values(), index=json_data.keys()).transpose()\n",
    "        # getting predictions from our model.\n",
    "        # it is much simpler because we used pipelines during development\n",
    "        res = model.predict_proba(df)\n",
    "        # we cannot send numpt array as a result\n",
    "        return res.tolist() \n",
    "    \n",
    "\n",
    "# Now, we need to assign an endpoint to our API.\n",
    "# assign endpoint\n",
    "api.add_resource(Scoring, '/scoring')\n",
    "\n",
    "\n",
    "# The last thing to do is to create an application run when the file api.py is run directly (not imported as a module from another script).\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n",
    "    \n",
    "    \n",
    "# Run the API by opening the command line and type python app.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "back to notebook, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "URL = \"http://127.0.0.1:5000/scoring\"\n",
    "# sending get request and saving the response as response object \n",
    "r = requests.post(url = URL, json = json_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
