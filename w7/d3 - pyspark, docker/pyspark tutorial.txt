First of all, you need to initiate a SparkContext.

	import pyspark
	from pyspark import SparkContext
	sc =SparkContext()

Now that the SparkContext is ready, you can create a collection of data called RDD, Resilient Distributed Dataset. Computation in an RDD is automatically parallelized across the cluster.

	nums= sc.parallelize([1,2,3,4])	

You can apply a transformation to the data with a lambda function. In the PySpark example below, you return the square of nums. It is a map transformation

	squared = nums.map(lambda x: x*x).collect()
	for num in squared:
    		print('%i ' % (num))


A more convenient way is to use the DataFrame. SparkContext is already set, you can use it to create the dataFrame. You also need to declare the SQLContext

SQLContext allows connecting the engine with different data sources. It is used to initiate the functionalities of Spark SQL.

	from pyspark.sql import Row
	from pyspark.sql import SQLContext

	sqlContext = SQLContext(sc)



[('John',19),('Smith',29),('Adam',35),('Henry',50)]
rdd = sc.parallelize(list_p)
rdd.map(lambda x: Row(name=x[0], age=int(x[1])))

sqlContext.createDataFrame(ppl)
list_p = [('John',19),('Smith',29),('Adam',35),('Henry',50)]
rdd = sc.parallelize(list_p)
ppl = rdd.map(lambda x: Row(name=x[0], age=int(x[1])))
DF_ppl = sqlContext.createDataFrame(ppl)