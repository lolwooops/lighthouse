{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "standard-fight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "forbidden-tablet",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'C:/Users/Tim/Desktop/lighthouse/w7/d3/'\n",
    "dataset = 'titanic_dataset.csv'\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "# df = pd.read_csv(filepath+dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "beneficial-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.option(\"inferSchema\",True).option(\"header\",True).csv(filepath+dataset)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "complex-quilt",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "processed-combination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def info(x):\n",
    "#     n_missing = x.isnull().sum().sort_values(ascending=False)\n",
    "#     p_missing = (x.isnull().sum()/x.isnull().count()).sort_values(ascending=False)\n",
    "#     dtype = x.dtypes\n",
    "#     count = x.count()\n",
    "#     missing_ = pd.concat([n_missing, p_missing, dtype, count],axis=1, keys = [\n",
    "#         'number_missing',\n",
    "#         'percent_missing',\n",
    "#         'type',\n",
    "#         'count'\n",
    "#     ])\n",
    "#     return missing_\n",
    "# info(df)\n",
    "\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "pregnant-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select([c for c in df.columns if c not in {'PassengerId','Cabin','Ticket','Name'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "military-switzerland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.69911764705882"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_val=df.select(mean(df.Age)).collect()\n",
    "mean_val[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "neutral-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(mean_val[0][0],subset=['Age'])\n",
    "df = df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "encouraging-domestic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+---+-----+-----+----+--------+\n",
      "|Survived|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+--------+------+---+---+-----+-----+----+--------+\n",
      "|       0|     0|  0|  0|    0|    0|   0|       0|\n",
      "+--------+------+---+---+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cross-rating",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+------+------------------+------------------+-------------------+-----------------+--------+\n",
      "|summary|           Survived|            Pclass|   Sex|               Age|             SibSp|              Parch|             Fare|Embarked|\n",
      "+-------+-------------------+------------------+------+------------------+------------------+-------------------+-----------------+--------+\n",
      "|  count|                889|               889|   889|               889|               889|                889|              889|     889|\n",
      "|   mean|0.38245219347581555|2.3115860517435323|  null|29.653446370674192|0.5241844769403825|0.38245219347581555|32.09668087739029|    null|\n",
      "| stddev|0.48625968831477334|0.8346997785705753|  null|12.968366309252314| 1.103704875596923| 0.8067607445174785|49.69750431670795|    null|\n",
      "|    min|                  0|                 1|female|              0.42|                 0|                  0|              0.0|       C|\n",
      "|    max|                  1|                 3|  male|              80.0|                 8|                  6|         512.3292|       S|\n",
      "+-------+-------------------+------------------+------+------------------+------------------+-------------------+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "personalized-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.select([c for c in df.columns if c not in {Survived}])\n",
    "# y = df.select('Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "renewable-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "incorrect-berry",
   "metadata": {},
   "outputs": [],
   "source": [
    "other = [\n",
    "    'Pclass',\n",
    "    'Parch',\n",
    "    'SibSp'\n",
    "]\n",
    "categoricals = [\n",
    "    'Sex',\n",
    "    'Embarked'\n",
    "]\n",
    "continuous = [\n",
    "    'Age',\n",
    "    'Fare'\n",
    "]\n",
    "\n",
    "stages = [] # stages in our Pipeline\n",
    "for col in categoricals:\n",
    "    stringIndexer = StringIndexer(inputCol=col, outputCol=col + \"Index\")\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()],\n",
    "                                     outputCols=[col + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "    \n",
    "# for concol in continuous:\n",
    "#     standardscaler = StandardScaler(inputCol=concol, outputCol = concol+'scaled', withStd = True)\n",
    "#     stages += [standardscaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "otherwise-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label into label indices using the StringIndexer\n",
    "label_stringIdx =  StringIndexer(inputCol=\"Survived\", outputCol=\"newlabel\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "visible-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "assemblerInput = other + [c + \"classVec\" for c in categoricals] + continuous#[d + \"scaled\" for d in continuous]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "italic-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=assemblerInput, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "electrical-design",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Survived=0, Pclass=3, Sex='male', Age=22.0, SibSp=1, Parch=0, Fare=7.25, Embarked='S'),\n",
       " Row(Survived=1, Pclass=1, Sex='female', Age=38.0, SibSp=1, Parch=0, Fare=71.2833, Embarked='C'),\n",
       " Row(Survived=1, Pclass=3, Sex='female', Age=26.0, SibSp=0, Parch=0, Fare=7.925, Embarked='S'),\n",
       " Row(Survived=1, Pclass=1, Sex='female', Age=35.0, SibSp=1, Parch=0, Fare=53.1, Embarked='S'),\n",
       " Row(Survived=0, Pclass=3, Sex='male', Age=35.0, SibSp=0, Parch=0, Fare=8.05, Embarked='S')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "infinite-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "model = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "floppy-morris",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "input_data = model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "adopted-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = sqlContext.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "protected-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|[3.0,0.0,1.0,1.0,...|\n",
      "|  1.0|[1.0,0.0,1.0,0.0,...|\n",
      "|  1.0|[3.0,0.0,0.0,0.0,...|\n",
      "|  1.0|[1.0,0.0,1.0,0.0,...|\n",
      "|  0.0|[3.0,0.0,0.0,1.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "golden-recognition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "df_train = df_train.repartition(100, 'label')\n",
    "train_data, test_data = df_train.randomSplit([.7,.3],seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "hollow-journalism",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|         405|\n",
      "|  1.0|         234|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "difficult-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "|label|count(label)|\n",
      "+-----+------------+\n",
      "|  0.0|         144|\n",
      "|  1.0|         106|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data.groupby('label').agg({'label': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cathedral-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "collected-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\",\n",
    "                        featuresCol=\"features\",\n",
    "                        maxIter=10,\n",
    "                        regParam=0.3)\n",
    "linearModel = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "continuing-squad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.30750717196605837,-0.017126025781781736,-0.052491181669684604,-1.022726360142806,-0.1667580499016104,0.22723265682069965,-0.009912410346669365,0.0024472705182523578]\n",
      "Intercept: 1.1340869380909364\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(linearModel.coefficients))\n",
    "print(\"Intercept: \" + str(linearModel.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "expressed-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "predictions = linearModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "hungry-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = predictions.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "behind-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|prediction|count(prediction)|\n",
      "+----------+-----------------+\n",
      "|       0.0|              192|\n",
      "|       1.0|               58|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compare results with test_data.groupby('label').agg({'label': 'count'}).show()\n",
    "cm.groupby('prediction').agg({'prediction': 'count'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "thousand-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 76.000%\n"
     ]
    }
   ],
   "source": [
    "# cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "\n",
    "def accuracy_m(model): \n",
    "    predictions = model.transform(test_data)\n",
    "    cm = predictions.select(\"label\", \"prediction\")\n",
    "    acc = cm.filter(cm.label == cm.prediction).count() / cm.count()\n",
    "    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \n",
    "accuracy_m(model = linearModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "circular-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8215408805031448\n",
      "areaUnderROC\n"
     ]
    }
   ],
   "source": [
    "### Use ROC \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "print(evaluator.evaluate(predictions))\n",
    "print(evaluator.getMetricName())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "communist-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.03, 0.1, 0.3, 1])\n",
    "             .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bacterial-emission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 182.580 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import *\n",
    "start_time = time()\n",
    "\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Run cross validations\n",
    "cvModel = cv.fit(train_data)\n",
    "# likely take a fair amount of time\n",
    "end_time = time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Time to train model: %.3f seconds\" % elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "weighted-desert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_56a5fc111f46', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_56a5fc111f46', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestModel = cvModel.bestModel\n",
    "bestModel.extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "overall-bryan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 76.000%\n"
     ]
    }
   ],
   "source": [
    "accuracy_m(model = cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "generous-security",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 76.000%\n"
     ]
    }
   ],
   "source": [
    "accuracy_m(model = bestModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-aaron",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
