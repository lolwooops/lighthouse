{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# with docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Keras has several datasets we can use to learn and, luckily for us, MNIST is among them! If you are not familiar with this dataset you can read more \n",
    "about it in the Wikipedia article MNIST database.\n",
    "\n",
    "Models and Layers are both modules that will help us build our NN (that is all you need to know for now:)) and to_categorical is used for our data \n",
    "encoding – but more on that later!\n",
    "\n",
    "Now that we have the required modules imported, we will want to split our dataset into train and test sets. This can be simply accomplished with the \n",
    "following line of code:\n",
    "\"\"\"\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# We haven't actually split the data in Python but we have loaded it to train and test samples directly from mnist.\n",
    "\n",
    "# Next, we can check the shape of our data to understand it better:\n",
    "\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In our train sample, we can see that we have 60,000 images with the size 28x28 pixels. We have 60,000 labels labeling the images from 0 to 9.\n",
    "\n",
    "We are now going to build our first network which will predict what number is in the picture:\n",
    "\"\"\"\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# Using the code above, we've created our first network. We did three layers but we will talk more about this during the following week. The goal of \n",
    "# this first tutorial is to show us that creating a network doesn't necessarily have be too complicated and it can take only a couple of rows of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before we can feed our data into our newly created model, we will need to reshape our input into a format that the model can read. The original shape of our input was \n",
    "[60000, 28, 28] which essentially represents 60,000 images with the pixel height and width of 28x28. We will reshape it, so that we have all pixels for each image \n",
    "in one row of a 2D array. We can think about this as a dataset with 60,000 rows and 28*28 columns.\n",
    "\"\"\"\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "# We also have to make sure our network thinks it is a categorical problem because numbers from 0 to 9 can be interpreted as \n",
    "# regression as well. So we will encode our target as categories:\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "With our dataset split into training and test sets, with our model compiled, and with our data reshaped and encoded, we are now ready to train our NN! To do this, \n",
    "we will call the fit function and pass in the required parameters:\n",
    "\"\"\"\n",
    "\n",
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "\"\"\"\n",
    "We pass in epochs, which dictate the number of backward and forward propagations (more about this during the upcoming week as well), and the batch_size, \n",
    "which indicates the number of training samples per backward/forward propagation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc, 'test_loss', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "And we add our NN layers. For this example, we will be using dense layers. A dense layer simply means that each neuron receives input from all the neurons in the \n",
    "previous layer. [784] and [10] refer to the dimensionality of the output space, we can think of this as the number of inputs for the subsequent layers, and since \n",
    "we are trying to solve a classification problem with 10 possible categories (numbers 0 to 9) the final layer has a potential output of 10 units. The activation \n",
    "parameter refers to the activation function we want to use, in essence, an activation function calculates an output based on a given input. \n",
    "And finally, the input shape of [28 * 28] refers to the image’s pixel width and height.\n",
    "\"\"\"\n",
    "\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(784, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Once our model is defined, and we have added our NN layers, we simply compile the model with our optimizer of choice, our loss function of choice, \n",
    "and the metrics we want to use to judge our model’s performance.\n",
    "\"\"\"\n",
    "\n",
    "network.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc, 'test_loss', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
