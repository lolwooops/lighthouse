{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Agenda\n",
    "Collecting data\n",
    "Importing libraries and splitting data to train and test\n",
    "Building the network\n",
    "Data augmentation\n",
    "Training\n",
    "Testing\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is the most important step in creating our network. It consists of four parts:\n",
    "\n",
    "Convolution\n",
    "Pooling\n",
    "Flattening\n",
    "Full connection\n",
    "\"\"\"\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = 'relu'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Convolution\n",
    "\n",
    "The primary purpose of convolution is to extract features from the input image. Convolution preserves the spatial relationship between pixels by learning image features \n",
    "using small squares of input data.\n",
    "\n",
    "First three parameters refer to:\n",
    "\n",
    "Filters: The dimensionality of the output space (i.e. the number of output filters in the convolution). We will end up with a convolved feature matrix of 32x32.\n",
    "Kernel_size: Specifying the height and width of the 2D convolution window. It can be a tuple if we don't want a square.\n",
    "Strides: Specifying the strides of the convolution along with the height and width. It can be a tuple if we want a different height and width.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Pooling\n",
    "\n",
    "In the case of Max Pooling, we use the parameter pool_size to define a spatial neighborhood (in our case a 2×2 window) and take the largest element from the rectified \n",
    "feature map within that window.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Flattening\n",
    "\n",
    "We convert the matrix into a 1D array which can be the input of the final Neural Network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Full Connection\n",
    "\n",
    "The full connection is connecting our convolutional network to a Neural Network that does final predictions.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Step 4 - Data Augmentation\n",
    "We need a lot of data to train a network but suppose we have a limited number of images for our network – what do we do? We don’t need to hunt for new images that can be \n",
    "added to our dataset. Why? Because neural networks aren’t smart, to begin with.\n",
    "\n",
    "So, to get more data, we just need to make alterations to our existing dataset – minor changes such as flips, translations, or rotations – and our neural network will think \n",
    "these are distinct images anyway. Data augmentation is a way of reducing overfitting of models, where we increase the amount of training data using only the information from \n",
    "our training data. The field of data augmentation is not new and, in fact, there are various data augmentation techniques for specific problems.\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Training\n",
    "\n",
    "history = classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 50,\n",
    "                         epochs = 10,\n",
    "                         validation_data = test_set)\n",
    "\n",
    "# Beware of overfitting in case of using too many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "We can see that even though the loss of the training set goes down quite fast, it is not the case with the test set. \n",
    "That means that our model doesn't generalize very well on new data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# loading an image from the disk\n",
    "test_image = image.load_img('random_dog.png', target_size = (64, 64))\n",
    "# converting the image to a numpy array\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "# print(training_set.class_indices)\n",
    "# our cut-off\n",
    "if result[0][0] >= 0.5:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
