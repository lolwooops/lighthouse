{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bag of Words Model — Find the unique words i.e., vocabulary from the list of documents. Parse each document word with the vocabulary, if present ‘1’ else ‘0’. \n",
    "This makes each document vector maintain the same length that of vocabulary length. We use this vocabulary for the new document vectorization.\n",
    "\"\"\"\n",
    "\n",
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \"I hate this phone\"]\n",
    "words = list(set([word for doc in docs for word in doc.lower().split()]))\n",
    "vectors = []\n",
    "for doc in docs:\n",
    "    vectors.append([1 if word in doc.lower().split() else 0 for word in words])\n",
    "print(\"vocabulary: \", words)   \n",
    "print(\"vectors: \", vectors)\n",
    "--------------------------------------------------------------------\n",
    "vocabulary:  ['am', 'hate', 'i', 'in', 'love', 'phone', 'superb,', 'this']\n",
    "vectors:  [[1, 0, 1, 1, 1, 1, 1, 1], [0, 1, 1, 0, 0, 1, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2. Word Counts with CountVectorizer(scikit-learn) — Tokenize the collection of documents and form a vocabulary with it and use this vocabulary to encode new documents. \n",
    "We can use CountVectorizer of the scikit-learn library. It by default remove punctuation and lower the documents.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of documents\n",
    "docs = ['SUPERB, I AM IN LOVE IN THIS PHONE', 'I hate this phone']\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(docs)\n",
    "# summarize encoded vector\n",
    "print('shape: ', vector.shape)\n",
    "print('vectors: ', vector.toarray())\n",
    "--------------------------------------------------------------------\n",
    "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
    "shape:  (2, 7)\n",
    "vectors:  [[1 0 2 1 1 1 1] [0 1 0 0 1 0 1]]\n",
    "    \n",
    "\"\"\"\n",
    "It turns each vector into the sparse matrix. It will make sure the word present in the vocabulary and if present it prints the number of occurrences of the word in the \n",
    "vocabulary. The word “in” present twice in the first document so ‘2’ appeared in the first vector.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "3. Word Frequencies with TfidfVectorizer (scikit-learn) — Word counts are pretty basic. In the first document, the word “in” has repeated and with that word we can’t draw \n",
    "any meaning. Stop words can repeat several times in a document and word count prioritize with the occurrence of the word. From word counts, we lose the interesting words and \n",
    "we mostly give priority to stopping words/less meaning carrying words.\n",
    "TF-IDF is a popular method. Acronym is “Term Frequency and Inverse Document Frequency”. TF-IDF is word frequency scores that try to highlight words that are more \n",
    "interesting, e.g. frequent in a document but not across documents.\n",
    "There are a few types of weighting schemes for tf-idf in general. Let's see how scikit-learn calculates tf*idf.\n",
    "From scikit-learn — “The actual formula used for tf-idf is tf * (idf + 1) = tf + tf * idf”. \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of documents\n",
    "docs = [\"SUPERB, I AM IN LOVE IN THIS PHONE\", \n",
    "        \"I hate this phone\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(docs)\n",
    "# summarize\n",
    "print('vocabulary: ', vectorizer.vocabulary_)\n",
    "print('idfs: ', vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([docs[0]])\n",
    "# summarize encoded vector\n",
    "print('vectors: ', vector.toarray())\n",
    "--------------------------------------------------------------\n",
    "vocabulary:  {'superb': 5, 'am': 0, 'in': 2, 'love': 3, 'this': 6, 'phone': 4, 'hate': 1}\n",
    "idfs:  [1.40546511 1.40546511 1.40546511 1.40546511 1.         1.40546511 1.        ]\n",
    "vectors:  [[0.35327777 0.         0.70655553 0.35327777 0.25136004 0.35327777 0.25136004]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The final step is vector normalization, scikit-learn uses ‘l2’ normalization technique for each document.\n",
    "The sum of squares of each value in the document vector is always 1\n",
    "\n",
    "\n",
    "\n",
    "Tf-idf is the best vectorization method among these three because it prioritises the words in each document. IDF value for the word “this” is less since it present in both \n",
    "the documents. So, unlike word counts which give a higher value for stop words like “in”, “this”, word frequency lowers the value if it present in more number of documents, \n",
    "because stop words repeat in each document almost.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
