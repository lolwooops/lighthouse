{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Choose a language model that will best represent the input text.\n",
    "Clean and prepare the data for training.\n",
    "Build a basic Keras sequential neural network model.\n",
    "Apply recurrent neural network (RNN) to process character sequences.\n",
    "Generate 3 channel RGB color outputs.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Language Model\n",
    "There are two general options for language modeling: word-level models and character-level models. Each has its advantages and disadvantages. Let’s go through them now.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Word-level Language Model\n",
    "The word-level language model can handle relatively long and clean sentences. By “clean”, I mean the words in the text datasets are free from typos and have few words \n",
    "outside of English vocabulary. The word-level language model encodes each unique word into a corresponding integer, and there’s a predefined fixed-sized vocabulary \n",
    "dictionary to look up the word to integer mapping. One major benefit of the word-level language model is its ability to leverage pre-trained word embeddings such as Word2Vec \n",
    "or GloVe. These embeddings represent words as vectors with useful properties. Words close in context are close in Euclidean distance and can be used to understand analogies \n",
    "like \"man is to women, as king is to queen\". Using these ideas, you can train a word-level model with relatively small labeled training sets.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Character Level Language Model\n",
    "\n",
    "But there’s an even simpler language model, one that splits a text string into characters and associates a unique integer to every single character. There are some reasons \n",
    "you might choose to use the character-level language model over the more popular word-level model:\n",
    "\n",
    "Your text datasets contain a noticeable amount of out-of-vocabulary words or infrequent words. In our case, some legitimate color names could be “aquatone”, “chartreuse” and \n",
    "“fuchsia”. For me, I have to check a dictionary to find out their meanings, and traditional word-level embeddings may not contain them.\n",
    "The majority of the text strings are short, bounded-length strings. If you’re looking for a specific length limit, I’ve been dealing with a Yelp review generation model with \n",
    "character level encode character length of 60 and still get decent results. You can find that blog post here: How to generate realistic yelp restaurant reviews with Keras. \n",
    "Usually, the character-level language generation model can create text with more variety since its imagination is not constrained by a pre-defined dictionary of vocabulary.\n",
    "You may also be aware of the limitation that came with adopting character-level language: - Long sequences may not capture long-range dependencies as well as word-level \n",
    "language models. - Character-level models are also more computationally expensive to train — given the same text data sets, these model sequences are longer and, as a result,\n",
    "require extended training time.\n",
    "\n",
    "Fortunately, these limitations won’t pose a threat to our color generation task. We’re limiting our color names to 25 characters in length and we only have 18,606 training \n",
    "samples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import preprocessing\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Dropout, LSTM, Reshape\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"colors.csv\")\n",
    "names = data[\"name\"]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data prep\n",
    "\n",
    "h = sorted(names.str.len().values)\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as plt\n",
    "\n",
    "fit = stats.norm.pdf(h, np.mean(h), np.std(h))  #this is a fitting indeed\n",
    "plt.plot(h,fit,'-o')\n",
    "plt.hist(h,normed=True)      #use this to draw histogram of your data\n",
    "plt.xlabel('Chars')\n",
    "plt.ylabel('Probability density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 25\n",
    "t = Tokenizer(char_level=True)\n",
    "t.fit_on_texts(names)\n",
    "tokenized = t.texts_to_sequences(names)\n",
    "padded_names = preprocessing.sequence.pad_sequences(tokenized, maxlen=maxlen)\n",
    "print(padded_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import np_utils\n",
    "one_hot_names = np_utils.to_categorical(padded_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization\n",
    "\n",
    "# The RGB values are between 0 - 255\n",
    "# scale them to be between 0 - 1\n",
    "def norm(value):\n",
    "    return value / 255.0\n",
    "\n",
    "normalized_values = np.column_stack([norm(data[\"red\"]), norm(data[\"green\"]), norm(data[\"blue\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(maxlen, 90)))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(one_hot_names, normalized_values,\n",
    "                    epochs=40,\n",
    "                    batch_size=32,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate colors\n",
    "\n",
    "# plot a color image\n",
    "def plot_rgb(rgb):\n",
    "    data = [[rgb]]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "def scale(n):\n",
    "    return int(n * 255) \n",
    "\n",
    "def predict(name):\n",
    "    name = name.lower()\n",
    "    tokenized = t.texts_to_sequences([name])\n",
    "    padded = preprocessing.sequence.pad_sequences(tokenized, maxlen=maxlen)\n",
    "    one_hot = np_utils.to_categorical(padded, num_classes=90)\n",
    "    pred = model.predict(np.array(one_hot))[0]\n",
    "    r, g, b = scale(pred[0]), scale(pred[1]), scale(pred[2])\n",
    "    print(name + ',', 'R,G,B:', r,g,b)\n",
    "    plot_rgb(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"forest\")\n",
    "predict(\"ocean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
