{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dimensionality Reduction\n",
    "\n",
    "Topic modeling is a form of dimensionality reduction. Rather than representing a text T in its feature space as {Word_i: count(Word_i, T) for Word_i in V}, we can \n",
    "represent the text in its topic space as {Topic_i: weight(Topic_i, T) for Topic_i in Topics}. Notice that we’re using Topics to represent the set of all topics.\n",
    "\n",
    "\n",
    "Unsupervised Learning\n",
    "\n",
    "Topic modeling can be easily compared to clustering. As in the case of clustering, the number of topics, like the number of clusters, is a hyperparameter. By doing topic \n",
    "modeling we build clusters of words rather than clusters of texts. \n",
    "A text is thus a mixture of all the topics, each having a certain weight.\n",
    "\n",
    "\n",
    "A Form of Tagging\n",
    "If document classification is assigning a single category to a text, topic modeling is assigning multiple tags to a text. A human expert can label the resulting topics \n",
    "with human-readable labels and use different heuristics to convert the weighted topics to a set of tags.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Text classification – Topic modeling can improve classification by grouping similar words together in topics rather than using each word as a feature\n",
    "Recommender Systems – Using a similarity measure we can build recommender systems. If our system would recommend articles for readers, it will recommend articles with \n",
    "a topic structure similar to the articles the user has already read.\n",
    "Uncovering Themes in Texts – Useful for detecting trends in online publications for example\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "LDA – Latent Dirichlet Allocation – The one we’ll be focusing in this tutorial. Its foundations are Probabilistic Graphical Models\n",
    "LSA or LSI – Latent Semantic Analysis or Latent Semantic Indexing – Uses Singular Value Decomposition (SVD) on the Document-Term Matrix. Based on Linear Algebra\n",
    "NMF – Non-Negative Matrix Factorization – Based on Linear Algebra\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USING GENSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    " \n",
    "data = []\n",
    " \n",
    "for fileid in brown.fileids():\n",
    "    document = ' '.join(brown.words(fileid))\n",
    "    data.append(document)\n",
    "\n",
    "NO_DOCUMENTS = len(data)\n",
    "print(NO_DOCUMENTS)\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim doesn’t have an implementation for NMF so we’re only going to play with LDA and LSI (Latent Semantic Indexing AKA Latent Semantic Analysis) models.\n",
    "\n",
    "import re\n",
    "from gensim import models, corpora\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "NUM_TOPICS = 10\n",
    "STOPWORDS = stopwords.words('english')\n",
    " \n",
    "def clean_text(text):\n",
    "    tokenized_text = word_tokenize(text.lower())\n",
    "    cleaned_text = [t for t in tokenized_text if t not in STOPWORDS and re.match('[a-zA-Z\\-][a-zA-Z\\-]{2,}', t)]\n",
    "    return cleaned_text\n",
    " \n",
    "# For gensim we need to tokenize the data and filter out stopwords\n",
    "tokenized_data = []\n",
    "for text in data:\n",
    "    tokenized_data.append(clean_text(text))\n",
    " \n",
    " \n",
    "# Build a Dictionary - association word to numeric id\n",
    "dictionary = corpora.Dictionary(tokenized_data)\n",
    " \n",
    "# Transform the collection of texts to a numerical form\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_data]\n",
    " \n",
    "# Have a look at how the 20th document looks like: [(word_id, count), ...]\n",
    "print(corpus[20])\n",
    "# [(12, 3), (14, 1), (21, 1), (25, 5), (30, 2), (31, 5), (33, 1), (42, 1), (43, 2),  ...\n",
    " \n",
    "# Build the LDA model\n",
    "lda_model = models.LdaModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)\n",
    " \n",
    "# Build the LSI model\n",
    "lsi_model = models.LsiModel(corpus=corpus, num_topics=NUM_TOPICS, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s now display the topics the two models have inferred:\n",
    "\n",
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    "\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(NUM_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
    "\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let’s now put the models to work and transform unseen documents to their topic distribution:\n",
    "\"\"\"\n",
    "\n",
    "text = \"The economy is working better than ever\"\n",
    "bow = dictionary.doc2bow(clean_text(text))\n",
    " \n",
    "print(lsi_model[bow])\n",
    "\n",
    "print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim offers a simple way of performing similarity queries using topic models.\n",
    "\n",
    "from gensim import similarities\n",
    " \n",
    "lda_index = similarities.MatrixSimilarity(lda_model[corpus])\n",
    " \n",
    "# Let's perform some queries\n",
    "similarities = lda_index[lda_model[bow]]\n",
    "# Sort the similarities\n",
    "similarities = sorted(enumerate(similarities), key=lambda item: -item[1])\n",
    " \n",
    "# Top most similar documents:\n",
    "print(similarities[:10])\n",
    "# [(104, 0.87591344), (178, 0.86124849), (31, 0.8604598), (77, 0.84932965), (85, 0.84843522), (135, 0.84421808), (215, 0.84184396), (353, 0.84038532), (254, 0.83498049), (13, 0.82832891)]\n",
    " \n",
    "# Let's see what's the most similar document\n",
    "document_id, similarity = similarities[0]\n",
    "print(data[document_id][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let’s now go through the same process with sklearn. This librabry offers a NMF implementation as well. The algorithms are more bare-bones than what we’ve seen with gensim \n",
    "but on the plus side, they implement the fit/transform interface we’re used with:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "NUM_TOPICS = 10\n",
    " \n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "                             stop_words='english', lowercase=True, \n",
    "                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    " \n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Non-Negative Matrix Factorization Model\n",
    "nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "nmf_Z = nmf_model.fit_transform(data_vectorized)\n",
    "print(nmf_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "# Build a Latent Semantic Indexing Model\n",
    "lsi_model = TruncatedSVD(n_components=NUM_TOPICS)\n",
    "lsi_Z = lsi_model.fit_transform(data_vectorized)\n",
    "print(lsi_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    " \n",
    "\n",
    "# Let's see how the first document in the corpus looks like in different topic spaces\n",
    "print(lda_Z[0])\n",
    "print(nmf_Z[0])\n",
    "print(lsi_Z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to inspect the inferred topics we need to implement a print function ourselves:\n",
    "\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, vectorizer)\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"NMF Model:\")\n",
    "print_topics(nmf_model, vectorizer)\n",
    "print(\"=\" * 20)\n",
    " \n",
    "print(\"LSI Model:\")\n",
    "print_topics(lsi_model, vectorizer)\n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming an unseen document goes like this:\n",
    "\n",
    "text = \"The economy is working better than ever\"\n",
    "x = nmf_model.transform(vectorizer.transform([text]))[0]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here’s how to implement the similarity functionality we’ve seen in the gensim section:\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    " \n",
    "def most_similar(x, Z, top_n=5):\n",
    "    dists = euclidean_distances(x.reshape(1, -1), Z)\n",
    "    pairs = enumerate(dists[0])\n",
    "    most_similar = sorted(pairs, key=lambda item: item[1])[:top_n]\n",
    "    return most_similar\n",
    " \n",
    "similarities = most_similar(x, nmf_Z)\n",
    "document_id, similarity = similarities[0]\n",
    "print(data[document_id][:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Plotting words and documents in 2D with SVD\n",
    "We can use SVD with 2 components (topics) to display words and documents in 2D. The process is really similar. Let’s start with displaying documents since it’s a bit more \n",
    "straightforward.\n",
    "\n",
    "In case you are running this in a Jupyter Notebook, run the following lines to init bokeh:\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=2)\n",
    "documents_2d = svd.fit_transform(data_vectorized)\n",
    " \n",
    "df = pd.DataFrame(columns=['x', 'y', 'document'])\n",
    "df['x'], df['y'], df['document'] = documents_2d[:,0], documents_2d[:,1], range(len(data))\n",
    " \n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"document\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "You can try going through the documents to see if indeed closer documents on the plot are more similar. \n",
    "To display words in 2D we just need to transpose the vectorized data: words_2d = svd.fit_transform(data_vectorized.T).\n",
    "\"\"\"\n",
    "\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "words_2d = svd.fit_transform(data_vectorized.T)\n",
    " \n",
    "df = pd.DataFrame(columns=['x', 'y', 'word'])\n",
    "df['x'], df['y'], df['word'] = words_2d[:,0], words_2d[:,1], vectorizer.get_feature_names()\n",
    " \n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    " \n",
    "plot = figure(plot_width=600, plot_height=600)\n",
    "plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "More about Latent Dirichlet Allocation\n",
    "LDA is the most popular method for doing topic modeling in real-world applications. That is because it provides accurate results, can be trained online \n",
    "(do not retrain every time we get new data) and can be run on multiple cores. Let’s repeat the process we did in the previous sections with sklearn and \n",
    "LatentDirichletAllocation:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "NUM_TOPICS = 10\n",
    " \n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, \n",
    "                             stop_words='english', lowercase=True, \n",
    "                             token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "data_vectorized = vectorizer.fit_transform(data)\n",
    " \n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(data_vectorized)\n",
    " \n",
    "text = \"The economy is working better than ever\"\n",
    "x = lda_model.transform(vectorizer.transform([text]))[0]\n",
    "print(x, x.sum())\n",
    "\n",
    "\"\"\"\n",
    "Notice how the factors corresponding to each component (topic) add up to 1. That’s not a coincidence. Indeed, LDA considers documents as being generated by a mixture of the \n",
    "topics. The purpose of LDA is to compute how much of the document was generated by which topic.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LDA is an iterative algorithm. Here are the two main steps:\n",
    "\n",
    "In the initialization stage, each word is assigned to a random topic.\n",
    "Iteratively, the algorithm goes through each word and reassigns the word to a topic taking into consideration:\n",
    "What’s the probability of the word belonging to a topic\n",
    "What’s the probability of the document to be generated by a topic\n",
    "Due to these important qualities, we can visualize LDA results easily. We’re going to use a specialized tool called PyLDAVis:\n",
    "\"\"\"\n",
    "\n",
    "import pyLDAvis.sklearn\n",
    " \n",
    "pyLDAvis.enable_notebook()\n",
    "panel = pyLDAvis.sklearn.prepare(lda_model, data_vectorized, vectorizer, mds='tsne')\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
