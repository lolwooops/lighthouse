{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import make_moons from sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# import DBSCAN\n",
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we have to create an instance of DBSCAN class from the Sklearn library:\n",
    "db = DBSCAN(eps=0.5,\n",
    "            min_samples=5,\n",
    "            metric='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We created the instance of DBSCAN class with a few parameters we didn't use before:\n",
    "\n",
    "eps: The maximum distance between two samples for one to be considered as being in the neighborhood of the other. \n",
    "This is not a maximum bound on the distances of points within a cluster. It is the most important DBSCAN parameter to \n",
    "choose appropriately for our dataset and distance function.\n",
    "\n",
    "min_samples: The number of samples in a neighborhood for a point to be considered as a core point. This includes the \n",
    "point itself.\n",
    "\n",
    "Now it's time to fit the data:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predicr\n",
    "y_db = db.fit_predict(X)\n",
    "\n",
    "# Plot DBSCAN clusters\n",
    "plot_clusters(X,y_db)\n",
    "\n",
    "#The difference from the algorithms used in the previous sections is that DBSCAN also created a separate cluster \n",
    "#for outliers. In the plot, the outliers are displayed as red squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But what happens when we try \"moon-shaped\" data? Let's find out:\n",
    "# generate moon-shape data\n",
    "X, y = make_moons(n_samples=200,\n",
    "                  noise=0.05,\n",
    "                  random_state=0)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#But how will k-means perform on data like this?\n",
    "\n",
    "# import k-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit K-means\n",
    "km = KMeans(n_clusters=2, # how many clusters we expected \n",
    "            n_init=10,\n",
    "            random_state=0,)\n",
    "\n",
    "y_km = km.fit_predict(X)\n",
    "\n",
    "# plot K-means clusters\n",
    "plot_clusters(X,y_km,plt_cluster_centers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the plot above, we can see that the result is not what we expected. The data is not separated into clusters well.\n",
    "\n",
    "#Can we get a better result from Agglomerative clustering? Let's find out:\n",
    "\n",
    "# import Agglomerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# fit Agglomerative clustering\n",
    "ac = AgglomerativeClustering(affinity='euclidean',\n",
    "                             linkage='ward',\n",
    "                             n_clusters = 2)\n",
    "y_hc = ac.fit_predict(X)\n",
    "\n",
    "# plot HC clusters\n",
    "plot_clusters(X,y_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The result is just as bad as with the k-means algorithm. In both cases, it is not our fault. \n",
    "#We set the parameters correctly and did our best to get good results. It's nice to remember that none of the clustering \n",
    "#algorithms mentioned above can deal with non-spherical clusters.\n",
    "\n",
    "#Lastly, let's try DBSCAN:\n",
    "# fit DBSCAN\n",
    "db = DBSCAN(eps=0.2,\n",
    "            min_samples=5,\n",
    "            metric='euclidean')\n",
    "\n",
    "y_db = db.fit_predict(X)\n",
    "\n",
    "# plot DBSCAN clusters\n",
    "plot_clusters(X,y_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "In this walkthrough, we have seen where it makes sense to use density models instead of more traditional, distance and hierarchical models. However, DBSCAN is not the best for sparse points that are still part of a cluster because they will be treated as outliers. We need to be careful about this when using density models in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
