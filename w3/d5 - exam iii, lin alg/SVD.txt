matrix decomposition
== method that reduces matrix into parts which makes it easier to do more complex matric operations. matric decomp methods (matric factorization) 

singular value decomp (svd) => decomping matrix into constituent elements
-all matrices have svd
-allows extraction of information



principal component analysis => transforms data linearly into new properties that are not correlated with each other



svd vs pca
svd diagonalizes matrix into special matrices that are easy to manipulate and analyze
pca skips less significant components


matrix diagonalization
A = VXV^-1

singular vectors + singular values
the matrix AA^t and A^tA 
-symmetrical
-square
-positive semidefinite (EV are 0 or positive)
-both matrices have same positive eigenvalues
-both have same rank r as A
covariance matrices are in this form
since they are symmetric, we can choose its EV to be orthonormal (perpendicular to each other with unit length)

let u = AA^t; v = A^tA; let u and v be singular vectors of A
both matrices have the same positive eigenvalues
the sqroot of these EVs are called singular values
-U^tU = I
-V^tV = I



SVD - states that any matrix A can be factorized as A = USV^t
where U and V are orthogonal matrices with orthonormal eigenvectors from AAt and AtA

S is a diagonal matrix with r elements equal to the root of the positive eigenvalues of AAt or AtA. the diagonal elements are composed of singular values

arrange EVs in different orders to produce U and V
to standardize the solution, we order the EVs such that vectors with higher EVs come before those with small values

comparing to eigendecomposition, SVD works on non square matrices. U and V are invertible for any matrix in SVD and they are orthonormal 

basically, SVD: A = USV^t
where U = (u1...um) where u = eigenvectors of AAt
V = (v1...vn) where v = eigenvectors of AtA
S = diagonal matrix composed of positive eigenvalues (big to small)

since matrix V is orthogonal, VtV = I
then A = USVt
AV = US
=> Av1 = s1u1
==> outer products of ui and vi
A = s1u1v1(t) + ... srurvr(t)

since u and v are unit vectors, we can ignore terms (suv(t)) with very small singular value s



moore penrose pseudoinverse
not all matrices are invertible; ml -> unlikely to find exact solution with the presence of noise
compute a pseudoinverse which minimizes the LSE 
then the solution for x can be estimated as 
	Ax = b
	x ~~ A(+)b

in linear regression problem, x is our linear model; A contains the training data; b contains corresponding labels
solve X by
	Ax=b
	(U,D,V) <- svd(A)
	A(+) = VD(+)U(t)
	x = A(+)b
where D(+) takes the reciprocal 1/xi of the non zero elements of D



variance and covariance
sample variance divided by n-1 instead of n in variance: 
limited size of smaples, sample mean is biased and correlated with samples. avg square distance from this mean will be smaller than from general population
-sample covariance divided by n-1 compensates for smaller value and can be proven to be an unbiased estimate for variance

covariance matrices
variance measures how a variable varies between itself while covariance is btw two variables
	COV = XX(t)/n if X is already zero centered

the diagonal elements hold the variances of individual variables and the non diagonal elements hold the covariance btw two variables


covariance matrix and svd
we can use svd to decompose the sample covariance matrix. when training ml model, we can perform a linear regression to form a new property rather than treating 2 properties as separated/correlated

there are a few properties about a sample covariance matrix under svd
-total variance of the data equals the trace of the sample covariance matrix S which equals the sum of squares of S's singular values
	-can calc ratio of variance lost if we dropped smaller s
	-reflects the amount of info lost if eliminated
-first evector u1 of S points to the most important direction of the data
	-quantifies typical ratio btw features
-error, calc'd as the sum of the perpendicular squared distance from the sample points to u1 is the minimum when svd is used


property
covariance matrices are not only symmetric but they are also positive semidefinite. because variance is positive or 0, u(t)Vu below is always greater or equal zero --> V is positive semidefinite


correlation matrix
scaled version of the covariance matrix. correlation matrix standardizes the variables to have an sd of 1

correlation matrix will be used if the variables are in scales of very different magnitudes; bad scaling may hurt ml algs like gradient desc



SVD factorizes matrix A into USV(t)
applying A toa vector x -> Ax can be visualized as performing a rotation (Vt), a scaling (S) and another rotation (U) on x

insight of SVD
since ui and vi have unit length, the most dominant factor in determining the significance of each term is the singular value si. we purposely sort si in desc order; if EVs become too small, we can ignore the remaining terms.
This formularization has some interesting implications. SVD decompose an nxn matric into r components with singular value si demonstrating its significance; consider this as a way to extract entangled and related properties into fewer principal directions with no correlations.

if data is highly correlated, we should expect many si values to be small and can be ignored.


