variable selection

pca creates k principal components that replace created features; shouldnt use both original and components together

idea behind VS is to pick existing features instead of generating new ones
-filter
-wrapper


supervised; evaluated based on performance of resulting model


filter
apply a statistical measure to assign a score to each features. the features are ranked byu the score and either selected to be kept or removed from a dataset. the methods are often univariate and consider the faetures independently, or with regards to the dependent variable

> set of all features -> selecting best subset -> learning alg -> perf

common filter methods used are
-too high variance
-too low variance
-feature similarity (based on r^2) 
-chi squared
-anova
-correlation coefficient scores

input variable numerical
output 
-numerical -> pearsons, spearmans
-categorical -> anova, kendalls
input variable categorical
output
-numerical -> anova, kendalls
-categorical -> chisquared, mutual information


wrapper 
consider selection of set of features as a search problem where different combos are prepped eval'd and compared to other combinations. a pred model is used to evaluate combo of features and assign a score based on model accuracy

set of all features -> (selecting best subset) [generate subset -> learning alg].repeat -> performance

-forward selection
-backward elimination
-stepwise selection

forward selection
-iterative method in which we keep adding a feature that best improves our model until addition of a new feature that does not improve model performance
-efficient for choosing small subset of features
-misses features whose usefulness req other features (feature synergy)

backward elim
-start with all features and remove the least significant feature at each iteration which improves model performance; repeated until no improvement is obs
-efficient for discarding small subset features
-preserves features whose usefulness req other features
-less efficient for computation; takes more time to fit models with all features than with one feature

stepwise
-start with 1 variable and keep adding afeature that best improves model; in ea step we check the features we have already selected if the removal of one of those features wont improve the model

(both are hill climbing search)
