{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will continue in the notebook from last week where we work with house prices prediction.\n",
    "\n",
    "We are going to implement elements for filter feature selectors based on the following criteria:\n",
    "\n",
    "Small variance\n",
    "One of each pair of features, which are correlated together more than x\n",
    "Before doing any transformations we will extract our target variable to keep it as it is. Even though we can do some \n",
    "transformations to it, it is a good practice to do it separately:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "y = df_numeric.SalePrice\n",
    "df_numeric.drop(\"SalePrice\",axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1: Removing Features With Small Variance\n",
    "First of all, we will remove the columns with very little variance. Small variance equals small predictive power because all \n",
    "houses have very similar values.\n",
    "\n",
    "For most of our variable selection, we can use methods from sklearn:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "vt = VarianceThreshold(0.1)\n",
    "df_transformed = vt.fit_transform(df_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fit_transform() in sklearn transforms an object from DataFrame to nd.array and we are losing column names, \n",
    "so we need to do some tricks to get them back!\n",
    "\n",
    "We don't need column names for modeling but it helps with the interpretation of modeling results.\n",
    "\"\"\"\n",
    "\n",
    "# columns we have selected\n",
    "# get_support() is method of VarianceThreshold and stores boolean of each variable in the numpy array.\n",
    "selected_columns = df_numeric.columns[vt.get_support()]\n",
    "# transforming an array back to a data-frame preserves column labels\n",
    "df_transformed = pd.DataFrame(df_transformed, columns = selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 2: Removing Correlated Features\n",
    "The goal of this part is to remove one feature from each highly correlated pair.\n",
    "\n",
    "We are going to do this in 3 steps:\n",
    "\n",
    "Calculate a correlation matrix\n",
    "Get pairs of highly correlated features\n",
    "Remove correlated columns\n",
    "\"\"\"\n",
    "\n",
    "# step 1\n",
    "df_corr = df_transformed.corr().abs()\n",
    "\n",
    "# step 2\n",
    "indices = np.where(df_corr > 0.8) \n",
    "    indices = [(df_corr.index[x], df_corr.columns[y]) for x, y in zip(*indices)\n",
    "                                    if x != y and x < y]\n",
    "\n",
    "# step 3\n",
    "for idx in indices: #each pair\n",
    "    try:\n",
    "        df_transformed.drop(idx[1], axis = 1, inplace=True)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "\"\"\"\n",
    "The code above will drop one column from each pair that is correlated at least 0.8. \n",
    "If this happens twice, use try-except block to allow the code to continue even when KeyError occurs.\n",
    "\n",
    "We can check the correlated columns by printing the indices:\n",
    "\"\"\"\n",
    "\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 3: Forward Regression\n",
    "We have removed the features with no information and correlated features. The last thing we can do before modeling is to \n",
    "select the k-best features in terms of the relationship with the target variable.\n",
    "We will use the forward wrapper method for that:\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "skb = SelectKBest(f_regression, k=10)\n",
    "X = skb.fit_transform(df_transformed, y)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "We need to import the SelectKBest method. Plus, we have to decide what algorithm we are going to use for the actual selection. \n",
    "Since we want to do a forward regression, we also imported f_regression. We could use some other technique if, for \n",
    "example, the target variable was categorical.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#Convert X back to a data-frame and assign back the correct column names.\n",
    "# this will give us the position of top 10 columns\n",
    "skb.get_support()\n",
    "# column names\n",
    "df_transformed.columns[skb.get_support()]\n",
    "X = pd.DataFrame(X,columns=df_transformed.columns[skb.get_support()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
