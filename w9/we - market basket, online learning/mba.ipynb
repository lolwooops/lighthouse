{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data/Online Retail.xlsx')\n",
    "df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.InvoiceNo.str.contains('C', na=False)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove these credit invoices, we can find all invoices with ‘C’ in them, and take the inverse of the results. That can be accomplished with the following line of code:\n",
    "df = df[~df['InvoiceNo'].str.contains('C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we are ready to start our market basket analysis. First, we’ll groupby the columns that we want to consider. For the purposes of this analysis, we’ll only look at the United Kingdom orders.\n",
    "market_basket = df[df['Country'] ==\"United Kingdom\"].groupby(\n",
    "                ['InvoiceNo', 'Description'])['Quantity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to hot encode the data and get 1 transaction per row to prepare to run our mlxtend analysis.\n",
    "\n",
    "\"\"\"\n",
    "MLxtend can be used anytime you want and it is my preferred approach for market basket analysis. That said, there’s an issue (as of the date of this article) \n",
    "with using pandas with large datasets when performing the step of unstacking the data with this line:\n",
    "\"\"\"\n",
    "\n",
    "market_basket = market_basket.sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "market_basket.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks like a bunch of zeros. What good is that? Well…its exactly what we want to see. We’ve encoded our data to show when a product is sold with another product. \n",
    "# If there is a zero, that means those products haven’t sold together. Before we continue, we want to convert all of our numbers to either a `1` or a `0` \n",
    "# (negative numbers are converted to zero, positive numbers are converted to 1). \n",
    "# We can do this encoding step with the following function:\n",
    "\n",
    "def encode_data(datapoint):\n",
    "    if datapoint <= 0:\n",
    "        return 0\n",
    "    if datapoint >= 1:\n",
    "        return 1\n",
    "    \n",
    "market_basket = market_basket.applymap(encode_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, lets find out which items are frequently purchased together. We do this by applying the mlxtend `apriori` fuuinction to our dataset.\n",
    "\n",
    "itemsets = apriori(market_basket, min_support=0.03, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The final step is to build your association rules using the mxltend `association_rules` function. You can set the metric that you are most interested in \n",
    "# (either `lift` or `confidence` and set the minimum threshold for the condfidence level (called `min_threshold`). The `min_threshold` can be thought of as the \n",
    "#  level of confidence percentage that you want to return. For example, if you set `min_threshold` to 1, you will only see rules with 100% confidence. \n",
    "# I usually set this to 0.7 to start with.\n",
    "\n",
    "rules = association_rules(itemsets, metric=\"lift\", min_threshold=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if memory problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, groupby\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = df[df['Country'] ==\"United Kingdom\"]\n",
    "orders = df_manual.set_index('InvoiceNo')['StockCode']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = orders.value_counts().to_frame(\"frequency\")\n",
    "statistics['support']  = statistics / len(set(orders.index)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s filter out any rows of data that doesn’t have support above our min_support level\n",
    "\n",
    "min_support=0.03 # same value we used above.\n",
    "\n",
    "items_above_support = statistics[statistics['support'] >= min_support].index\n",
    "orders_above_support = orders[orders.isin(items_above_support)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We next need to filter out orders that only had 1 items ordered on the invoice, since those items won’t provide any insight into our market basket analysis.\n",
    "\n",
    "order_counts = orders.index.value_counts()\n",
    "orders_over_two_index = order_counts[order_counts>=2].index\n",
    "orders_over_two = orders[orders.index.isin(orders_over_two_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = orders_over_two.value_counts().to_frame(\"frequency\")\n",
    "statistics['support']  = statistics / len(set(orders_over_two.index)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to do the fun stuff. Calculating the itemsets / item pairs. We’ll create a function that will generate our itemsets and then send our new order dataset through \n",
    "# the generator. Then, we calculate the frequency of each item with each other (named `frequencyAC`) as well as the support (named `supportAC`). \n",
    "# Finally, we filter out the itemsets that are below our `min_support` level\n",
    "\n",
    "def itemset_generator(orders):\n",
    "    orders = orders.reset_index().values\n",
    "    for order_id, order_object in groupby(orders, lambda x: x[0]):\n",
    "        item_list = [item[1] for item in order_object]\n",
    "        for item_pair in combinations(item_list, 2):\n",
    "            yield item_pair\n",
    "\n",
    "itemsets_gen = itemset_generator(orders_over_two)\n",
    "itemsets  = pd.Series(Counter(itemsets_gen)).to_frame(\"frequencyAC\")\n",
    "itemsets['supportAC'] = itemsets['frequencyAC'] / len(orders_over_two_index) * 100\n",
    "itemsets = itemsets[itemsets['supportAC'] >= min_support]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can calculate our association rules. First, let’s unstack our itemsets and create the necessary data columns for support, lift, etc.\n",
    "\n",
    "# Create table of association rules and compute relevant metrics\n",
    "itemsets = itemsets.reset_index().rename(columns={'level_0': 'antecedents', 'level_1': 'consequents'})\n",
    "\n",
    "itemsets = (itemsets\n",
    "     .merge(statistics.rename(columns={'freq': 'freqA', 'support': 'antecedent support'}), left_on='antecedents', right_index=True)\n",
    "     .merge(statistics.rename(columns={'freq': 'freqC', 'support': 'consequents support'}), left_on='consequents', right_index=True))\n",
    "\n",
    "\n",
    "itemsets['confidenceAtoC'] = itemsets['supportAC'] / itemsets['antecedent support']\n",
    "itemsets['confidenceCtoA'] = itemsets['supportAC'] / itemsets['consequents support']\n",
    "itemsets['lift'] = itemsets['supportAC'] / (itemsets['antecedent support'] * itemsets['consequents support'])\n",
    "\n",
    "itemsets=itemsets[['antecedents', 'consequents','antecedent support', 'consequents support', 'confidenceAtoC','lift']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = itemsets\n",
    "rules_over_50 = rules[(rules.confidenceAtoC >0.50)]\n",
    "rules_over_50.set_index('antecedents',inplace=True)\n",
    "rules_over_50.reset_index(inplace=True)\n",
    "rules_over_50=rules_over_50.sort_values('lift', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
