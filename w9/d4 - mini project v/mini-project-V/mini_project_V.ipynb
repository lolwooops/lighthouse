{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `nltk` we can download translated sentences between different languages. You can see the example between **English and French** below but feel free to try different combination as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package comtrans to\n",
      "[nltk_data]     C:\\Users\\Tim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package comtrans is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('comtrans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AlignedSent: 'Resumption of the se...' -> 'Reprise de la sessio...'>\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import comtrans\n",
    "print(comtrans.aligned_sents('alignment-en-fr.txt')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comtrans.aligned_sents('alignment-en-fr.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = [line.words for line in comtrans.aligned_sents('alignment-en-fr.txt')]\n",
    "french = [line.mots for line in comtrans.aligned_sents('alignment-en-fr.txt')]\n",
    "\n",
    "data = []\n",
    "for i in range(len(english)):\n",
    "    eng = ' '.join(english[i])\n",
    "    fre = ' '.join(french[i])\n",
    "    data.append([eng,fre])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Resumption of the session', 'Reprise de la session'],\n",
       " ['I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999 , and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .',\n",
       "  'Je déclare reprise la session du Parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances .'],\n",
       " ['You have requested a debate on this subject in the course of the next few days , during this part-session .',\n",
       "  'Vous avez souhaité un débat à ce sujet dans les prochains jours , au cours de cette période de session .'],\n",
       " [\"Please rise , then , for this minute ' s silence .\",\n",
       "  'Je vous invite à vous lever pour cette minute de silence .'],\n",
       " [\"( The House rose and observed a minute ' s silence )\",\n",
       "  '( Le Parlement , debout , observe une minute de silence )']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import shuffle\n",
    "import copy\n",
    "import string\n",
    "import re\n",
    "\n",
    "from unicodedata import normalize\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33334, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep\n",
    "#sample\n",
    "#tokenize + pad\n",
    "#embed\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(file):\n",
    "    normalized_documents = []\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for pair in file:\n",
    "        cleaned = list()\n",
    "        for line in pair:\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            line = line.split()\n",
    "            line = [i.lower() for i in line]\n",
    "            line = [i.translate(table) for i in line]\n",
    "            line = [i for i in line if i.isalpha()]\n",
    "#             # Remove extra space characters\n",
    "#             line = re.sub(r'\\s+', ' ', line)\n",
    "#             # Remove distracting characters\n",
    "#             line = re.sub(r'''[\\*\\~]+''', \"\", line)\n",
    "            cleaned.append(' '.join(line))\n",
    "\n",
    "        normalized_documents.append(cleaned)\n",
    "        \n",
    "    normalized_documents = np.array(normalized_documents)\n",
    "    return normalized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = clean(data)\n",
    "\n",
    "data = copy.deepcopy(cleaned_data)\n",
    "shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['the resolution makes clear there is a need for a proportional system that gives form to women s representation and that all political parties must take action',\n",
       "        'la resolution dit clairement que le systeme proportionnel qui permet la representation des femmes est necessaire et que les partis politiques doivent agir en ce sens'],\n",
       "       ['i hope that turkey begins to play a constructive role in the cyprus issue',\n",
       "        'j espere que la turquie commencera a jouer un role constructif dans la question chypriote'],\n",
       "       ['should this be condemned ecologically frau breyer',\n",
       "        'madame breyer ce qui se passe dans ce pays estil critiquable sur le plan ecologique'],\n",
       "       ...,\n",
       "       ['proposal for a resolution by mr dupuis and others on behalf of the tdi group on the un commission on human rights',\n",
       "        'proposition de resolution deposee par les deputes dupuis et autres au nom du groupe tdi sur la commission des droits de l homme des nations unies'],\n",
       "       ['many in parliament believe that it is the country of origin that should be on the label',\n",
       "        'un grand nombre de deputes du parlement estiment que le pays d origine doit etre mentionne'],\n",
       "       ['secondly when dismissals prove to be inevitable the utmost must be done to reinforce the employability of those concerned',\n",
       "        'deuxiemement lorsque les licenciements sont inevitables les entreprises doivent faire leur possible pour renforcer l employabilite des personnes concernees']],\n",
       "      dtype='<U274')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = []\n",
    "output_ = []\n",
    "output_input = []\n",
    "\n",
    "count = 0\n",
    "max_num = 10000 #change to sample\n",
    "\n",
    "for i in data:\n",
    "    count += 1\n",
    "    if count > max_num:\n",
    "        break\n",
    "    input_sentence = i[0]\n",
    "    output = i[1]\n",
    "    output_sentence = output + ' <eos>'\n",
    "    output_sentence_input = '<sos> ' + output\n",
    "    \n",
    "    input_.append(input_sentence)\n",
    "    output_.append(output_sentence)\n",
    "    output_input.append(output_sentence_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples input: 10000\n",
      "num samples output: 10000\n",
      "num samples output input: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"num samples input:\", len(input_))\n",
    "print(\"num samples output:\", len(output_))\n",
    "print(\"num samples output input:\", len(output_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing\n",
    "def tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "def tokenizer2(lines2):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines2)\n",
    "    return tokenizer\n",
    "def tokenizer3(lines3):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines3)\n",
    "    return tokenizer\n",
    "def tokenizer4(lines4):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines4)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# def max_length(lines):\n",
    "#     return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokenizer = tokenizer(input_)\n",
    "# input_is = input_tokenizer.texts_to_sequences(input_) # is = integer seq\n",
    "input_index = input_tokenizer.word_index\n",
    "num_words_input = len(input_index) + 1\n",
    "\n",
    "\n",
    "\n",
    "output_tokenizer = tokenizer2(output_ + output_input)\n",
    "# output_is = output_tokenizer.texts_to_sequences(output_)\n",
    "# output_input_is = output_tokenizer.texts_to_sequences(output_input)\n",
    "output_index = output_tokenizer.word_index\n",
    "num_words_output = len(output_index) + 1\n",
    "output_tokenizer2 = tokenizer3(output_)\n",
    "output_tokenizer3 = tokenizer4(output_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "\n",
    "# encode_input = pad_sequences(input_is, maxlen = max_input_length)\n",
    "# print(\"encoder_input_sequences.shape:\", encoder_input.shape)\n",
    "\n",
    "# decoder_input = pad_sequences(output_input_is, maxlen = max_output_length, padding = 'post')\n",
    "# print(\"decoder_input_sequences.shape:\", decoder_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding\n",
    "\n",
    "def padding(tokenizer, length, lines, padding):\n",
    "    sequences = tokenizer.texts_to_sequences(lines)\n",
    "    sequences = pad_sequences(sequences,maxlen=length,padding=padding)\n",
    "    return sequences\n",
    "\n",
    "def encode_output(seq, sen_length):\n",
    "    ylist = list()\n",
    "    for i in seq:\n",
    "        encoded = to_categorical(seq, num_classes=sen_length)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(seq.shape[0], seq.shape[1], sen_length)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = max(len(sen) for sen in input_tokenizer.texts_to_sequences(input_))\n",
    "max_output_length = max(len(sen) for sen in output_tokenizer.texts_to_sequences(output_))\n",
    "\n",
    "encoder_input = padding(input_tokenizer, max_input_length, input_, 'pre')\n",
    "decoder_input = padding(output_tokenizer, max_output_length, output_, 'post')\n",
    "y = padding(output_tokenizer, max_output_length, output_, 'post')\n",
    "# decoder_output = encode_output(y, len(output_index)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 40, 12869)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_),\n",
    "        max_output_length,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")\n",
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,d in enumerate(y):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i,t,word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs -->\n",
    "# model - encoder\n",
    "# embedding\n",
    "# lstm\n",
    "# output_inputs -->\n",
    "# model - decoder\n",
    "# embedding\n",
    "# lstm\n",
    "# outputs -->\n",
    "# model - dense\n",
    "# compile\n",
    "# --> predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(max_num, len(input_index) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model from walkthrough\n",
    "\n",
    "# encoder\n",
    "#embed layer\n",
    "encoder_inp_plc = Input(shape=(max_input_length,))\n",
    "embedding_layer = Embedding(num_words_input, 100, input_length=max_input_length)\n",
    "x = embedding_layer(encoder_inp_plc)\n",
    "#lstm layer\n",
    "encoder = LSTM(256,return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h,c]\n",
    "\n",
    "#decoder\n",
    "#embed layer\n",
    "decoder_inp_plc = Input(shape=(max_output_length,))\n",
    "decoder_embedding = Embedding(num_words_output, 256)\n",
    "decoder_inp_x = decoder_embedding(decoder_inp_plc)\n",
    "#lstm layer\n",
    "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inp_x, initial_state=encoder_states)\n",
    "\n",
    "#dense layer - predict decoder outputs\n",
    "decoder_dense = TimeDistributed(Dense(num_words_output, activation = 'softmax'))\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#model compilation\n",
    "model = Model([encoder_inp_plc, decoder_inp_plc], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 - 280s - loss: 4.4361 - accuracy: 0.4816 - val_loss: 3.5080 - val_accuracy: 0.4926\n",
      "Epoch 2/10\n",
      "125/125 - 279s - loss: 3.2472 - accuracy: 0.5215 - val_loss: 3.0072 - val_accuracy: 0.5390\n",
      "Epoch 3/10\n",
      "125/125 - 275s - loss: 2.7408 - accuracy: 0.5699 - val_loss: 2.5288 - val_accuracy: 0.6195\n",
      "Epoch 4/10\n",
      "125/125 - 293s - loss: 2.1814 - accuracy: 0.6913 - val_loss: 1.9746 - val_accuracy: 0.7402\n",
      "Epoch 5/10\n",
      "125/125 - 298s - loss: 1.7183 - accuracy: 0.7693 - val_loss: 1.6264 - val_accuracy: 0.7874\n",
      "Epoch 6/10\n",
      "125/125 - 293s - loss: 1.4179 - accuracy: 0.8100 - val_loss: 1.3838 - val_accuracy: 0.8311\n",
      "Epoch 7/10\n",
      "125/125 - 284s - loss: 1.1894 - accuracy: 0.8484 - val_loss: 1.1896 - val_accuracy: 0.8633\n",
      "Epoch 8/10\n",
      "125/125 - 281s - loss: 1.0058 - accuracy: 0.8765 - val_loss: 1.0415 - val_accuracy: 0.8862\n",
      "Epoch 9/10\n",
      "125/125 - 295s - loss: 0.8623 - accuracy: 0.8971 - val_loss: 0.9262 - val_accuracy: 0.9030\n",
      "Epoch 10/10\n",
      "125/125 - 301s - loss: 0.7488 - accuracy: 0.9120 - val_loss: 0.8325 - val_accuracy: 0.9157\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(\n",
    "    [encoder_input, decoder_input],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    verbose = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_10 (InputLayer)           [(None, 39)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_7 (Embedding)         (None, 39, 100)      1008600     input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 40, 256)      3294464     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 256), (None, 365568      embedding_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 40, 256), (N 525312      embedding_8[0][0]                \n",
      "                                                                 lstm_8[0][1]                     \n",
      "                                                                 lstm_8[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_4 (TimeDistrib (None, 40, 12869)    3307333     lstm_9[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 8,501,277\n",
      "Trainable params: 8,501,277\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inp_plc,encoder_states)\n",
    "decoder_h = Input(shape=(256,))\n",
    "decoder_c = Input(shape=(256,))\n",
    "decoder_states_inputs = [decoder_h, decoder_c]\n",
    "\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "decoder_outputs, h, c = decoder_lstm(decoder_x, initial_state= decoder_states_inputs)\n",
    "decoder_states = [h,c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs_single] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(decoder_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word_inp = {v:k for k, v in input_index.items()}\n",
    "index2word_out = {v:k for k,v in output_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sq):\n",
    "    value = encoder_model.predict(input_sq)\n",
    "    target_sq = np.zeros((1,1))\n",
    "    target_sq[0,0] = output_index['sos']\n",
    "    eos = output_index['eos']\n",
    "    output_sentence = []\n",
    "    \n",
    "    for _ in range(max_output_length):\n",
    "        output_tokens, h, c = decoder_model.predict([target_sq]+value)\n",
    "        idx = np.argmax(output_tokens[0,0,:])\n",
    "        \n",
    "        if eos == idx:\n",
    "            break\n",
    "        word = ''\n",
    "        \n",
    "        if idx > 0:\n",
    "            word = index2word_out[idx]\n",
    "            output_sentence.append(word)\n",
    "        \n",
    "        target_sq[0,0]=idx\n",
    "        value = [h,c]\n",
    "    return ' '.join(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:  madam president for the past six months to a year the committee on development and cooperation has been calling on the commission to do something about the drought in ethiopia\n",
      "Response:  permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi permettezmoi monsieur monsieur monsieur monsieur monsieur monsieur monsieur moi monsieur moi monsieur moi monsieur quelles monsieur eu egalement eu egalement conseil egalement conseil\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_))\n",
    "input_sq = encoder_input[i:i+1]\n",
    "translation = translate(input_sq)\n",
    "print('Input: ', input_[i])\n",
    "print('Response: ', translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some rights will apply to anyone present in the eu territory while others will benefit only citizens of the european union'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "m = input_[10]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<sos> certains s appliqueront a toute personne presente sur le territoire de l union d autres auront pour seuls beneficiaires les citoyens de l union'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = output_input[10]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,   89,   80,   22,  562,\n",
       "          2,  751,  179,    5,    1,  105, 1192,  537,  382,   22,  752,\n",
       "         73,  189,    3,    1,   26,   48])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [m]\n",
    "temp = padding(input_tokenizer, max_input_length, test, 'pre')\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5,  151,   37, 7111,    3,  131,  511,  292,   28,    7,  984,\n",
       "          1,    6,   51,   12,   82, 1226,   22, 1388, 2121,    8,  160,\n",
       "          1,    6,   51,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = [n]\n",
    "temp2 = padding(output_tokenizer, max_output_length, test2, 'post')\n",
    "temp2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict([temp[0].reshape((1, temp[0].shape[0])),temp2[0].reshape((1, temp2[0].shape[0]))], verbose=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [np.argmax(vector) for vector in res]\n",
    "#integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applaudissements certains s signaler a toute personne presente sur le territoire de l union d autres auront pour seuls reelles les citoyens de l union eos eos eos eos eos eos eos eos eos eos\n"
     ]
    }
   ],
   "source": [
    "target = list()\n",
    "for i in integers:\n",
    "    word = word_for_id(i, output_tokenizer)\n",
    "    eos = output_index['eos']\n",
    "    if word is None:\n",
    "        break\n",
    "    if eos == integers:\n",
    "        break\n",
    "    target.append(word)\n",
    "translated = ' '.join(target)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "\n",
    "def define_model(inp_vocab, out_vocab, inp_timesteps, out_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(inp_vocab, n_units, input_length=inp_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 39, 256)           2582016   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector (RepeatVector) (None, 40, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 40, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 40, 12869)         3307333   \n",
      "=================================================================\n",
      "Total params: 6,939,973\n",
      "Trainable params: 6,939,973\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = define_model(num_words_input, num_words_output, max_input_length, max_output_length, 256)\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# summarize defined model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "125/125 - 252s - loss: 4.6230 - accuracy: 0.4804 - val_loss: 4.0521 - val_accuracy: 0.4854\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.05212, saving model to e2f.h5\n",
      "Epoch 2/15\n",
      "125/125 - 262s - loss: 3.9253 - accuracy: 0.4846 - val_loss: 3.8101 - val_accuracy: 0.4852\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.05212 to 3.81005, saving model to e2f.h5\n",
      "Epoch 3/15\n",
      "125/125 - 302s - loss: 3.7063 - accuracy: 0.4828 - val_loss: 3.6953 - val_accuracy: 0.4828\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.81005 to 3.69534, saving model to e2f.h5\n",
      "Epoch 4/15\n",
      "125/125 - 263s - loss: 3.6004 - accuracy: 0.4872 - val_loss: 3.6305 - val_accuracy: 0.4930\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.69534 to 3.63049, saving model to e2f.h5\n",
      "Epoch 5/15\n",
      "125/125 - 263s - loss: 3.5288 - accuracy: 0.4958 - val_loss: 3.6161 - val_accuracy: 0.4986\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.63049 to 3.61612, saving model to e2f.h5\n",
      "Epoch 6/15\n",
      "125/125 - 244s - loss: 3.4810 - accuracy: 0.5007 - val_loss: 3.6143 - val_accuracy: 0.5009\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.61612 to 3.61432, saving model to e2f.h5\n",
      "Epoch 7/15\n",
      "125/125 - 245s - loss: 3.4506 - accuracy: 0.5043 - val_loss: 3.5863 - val_accuracy: 0.5030\n",
      "\n",
      "Epoch 00007: val_loss improved from 3.61432 to 3.58627, saving model to e2f.h5\n",
      "Epoch 8/15\n",
      "125/125 - 240s - loss: 3.4254 - accuracy: 0.5057 - val_loss: 3.5848 - val_accuracy: 0.5014\n",
      "\n",
      "Epoch 00008: val_loss improved from 3.58627 to 3.58483, saving model to e2f.h5\n",
      "Epoch 9/15\n",
      "125/125 - 240s - loss: 3.4012 - accuracy: 0.5064 - val_loss: 3.6016 - val_accuracy: 0.5012\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 3.58483\n",
      "Epoch 10/15\n",
      "125/125 - 241s - loss: 3.3878 - accuracy: 0.5073 - val_loss: 3.6049 - val_accuracy: 0.5001\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 3.58483\n",
      "Epoch 11/15\n",
      "125/125 - 239s - loss: 3.3777 - accuracy: 0.5079 - val_loss: 3.5967 - val_accuracy: 0.4987\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 3.58483\n",
      "Epoch 12/15\n",
      "125/125 - 239s - loss: 3.3698 - accuracy: 0.5087 - val_loss: 3.6017 - val_accuracy: 0.4965\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 3.58483\n",
      "Epoch 13/15\n",
      "125/125 - 227s - loss: 3.3641 - accuracy: 0.5097 - val_loss: 3.6147 - val_accuracy: 0.4894\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.58483\n",
      "Epoch 14/15\n",
      "125/125 - 231s - loss: 3.3540 - accuracy: 0.5106 - val_loss: 3.6214 - val_accuracy: 0.4883\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.58483\n",
      "Epoch 15/15\n",
      "125/125 - 228s - loss: 3.3474 - accuracy: 0.5119 - val_loss: 3.6248 - val_accuracy: 0.4988\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.58483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2db153d8100>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('e2f.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit([encoder_input, decoder_input],\n",
    "          decoder_targets_one_hot, \n",
    "          epochs=15, \n",
    "          batch_size=64, \n",
    "          validation_split = 0.2, \n",
    "          callbacks=[checkpoint], \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'it is caught between the devil and the deep blue sea'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = input_[10]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,   14,    6, 3673,  126,    1,\n",
       "       5926,    4,    1, 1651, 4478, 1432])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [m]\n",
    "temp = padding(input_tokenizer, max_input_length, test, 'pre')\n",
    "temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4904, 6354, 2938, 2938,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = [m]\n",
    "temp2 = padding(output_tokenizer, max_input_length, test, 'post')\n",
    "temp2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(temp[0].reshape((1, temp[0].shape[0])), verbose=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = [np.argmax(vector) for vector in res]\n",
    "#integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circonstances circonstances circonstances circonstances circonstances circonstances circonstances franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement franchement\n"
     ]
    }
   ],
   "source": [
    "target = list()\n",
    "for i in integers:\n",
    "    word = word_for_id(i, output_tokenizer)\n",
    "    if word is None:\n",
    "        break\n",
    "    target.append(word)\n",
    "translated = ' '.join(target)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
